---
layout: post
title: On College
date: 2014-05-18
---

Some quick anchor links if you want to skip around:

#Bad Things:
- [Poor cross-discipline integration and irrelevance](#integration-irrelevance)
- [Cost: money](#cost-money)
- [Cost: time](#cost-time)

<br />
#Good Things:
- [Lots of practice writing](#writing)
- [Practice navigating organizational politics](#politics)
- [Well-roundedness](#rounded)
- [Something to be dissatisfied with](#dissatisfaction)
- [Free time to pursue other stuff](#free-time)
- [Socializing](#socializing)

<br />
#Bad Things

###<a name="integration-irrelevance"></a>Poor cross-discipline integration && lack of relevance
(There are only three "Bad Things" points, but this one is quite bad, so it at
least evens out.)

The college I went to ([Gustavus Adolphus College](http://gustavus.edu)), prides
itself on its cross-disciplinary writing program. The program effectively means
students in the hard sciences, education, nursing, and others must at least
sample the level and amount of writing that social science students do. Making
engineers better communicators is obviously a noble endeavor, and in my
opinion it is seemed to be effective. Sadly, the opposite program -- helping writer-types
become better engineers -- is nonexistent.

All Gustavus students fulfill certain minimum math and science requirements, but many are
bullshit: maybe an intro geology course or "The Nature of Math", which is about
as floaty and insubstantial as you can get while still doing math. The excuse
that "creative writing students just don't like computer science" or "students can't be
asked to specialize in something unrelated to their major" isn't good enough.
Being employable in 2014 means more than being able to balance a checkbook or write well.

If Gustavus and other liberal arts colleges had a modicum of their
heads remaining outside their asses, they would *require* all first and second
year students to take data/code literacy courses.

I'll even do them the favor of created the courses right now: let's call them
Scripting I & II.  Students would learn practical, applicable computering:
things like how to write their own Python programs to make custom calculators
for their discipline, how to automate simple tasks, how to make
a simple website to collaborate with group members or present findings, how to
use a browser's developer tools to inspect HTML/CSS/JS, maybe some version
control basics, and how to use a database to interact with research data
*firsthand*. Even if they come away without much in the way of genuine
programming ability, students would gain an appreciation for algorithmic
thinking and reverence for data as something they can manipulate themselves,
rather than relying on study authors to parse for them.

It would be required that all students majoring in Computer Science tutor a
given number of hours for Scripting I & II, to see what non-programmers do with
programs, how they learn, and how to become better teachers of those who know
less about machines than they do.

I majored in political science, and all majors were required to take a single
-- one! -- course on data analysis. This course was dated, impractical, and
involved no code or database work at all. If this doesn't strike you as rather
1980s, I'm afraid we may not see eye to bionic eye on what things are actually
like outside of the classroom. The need for these skills is not going away. This
trend will not reverse. Going forward, not knowing databases, basic scripting,
and basic HTML/CSS will hurt you, period.

(Many organizations, like [Girl Develop It](http://www.girldevelopit.com/), are
working to give people these skills. You should volunteer your talents if you
can.)

Teaching kids to type used to be a revolutionary idea, seen as relevant to a
future where computers would dominate. Guess what: they do dominate. That future
arrived in...1999? 2000? And our tech education -- for university students and
kindergarteners alike -- has hardly changed. Not everyone has to be programmers,
but everyone must have some of understanding about why and how computers have
changed the way we interact with the world.

The longer we whine and apologize for "not everyone needing to know how to use
technology" the larger the problem becomes. It is one of mindset, not ability.
If you want an example, look at journalism: an industry with no shortage of
exciting content and creativity has been brought low because many in the
community thought they were going to be selling classified ads and printing on
dead trees forever.

As multifaceted organizations that teach things as varied as sociology and
physics, Universities are the prime organizations for turning out the kind of
cross-pollinated people whose talents include "\_\_\_\_\_ and coding". College
can achieve the relevance it had 50 years ago, if we so desire it. Otherwise
people will eventually catch on that 100k+ and 4 years of youth is rather
expensive given the outcomes.

###<a name="cost-money"></a>Cost: money
I don't have to tell a goddamn soul about how expensive college is. Short of
apartheid or genocide, I'm not sure a nation could have come up with a better plan
to depress its future economic output than to convince its young people to take
on trillions of dollars of debt for, at best, dubious returns. Somewhere along
the line we, as a people, went and told cost-benefit analysis to fuck off. And
people have the nerve to come to me incredulous that [Dev Bootcamp](http://devbootcamp.com/)
cost me USD$12,000.

###<a name="cost-time"></a>Cost: time
While money is a cost, so too is time. College is an easy sell because most 17
year olds don't have any concept of what else they could productively do for 4
years. The fact that payment comes via student loans has a mitigating effect for
many, delaying the inevitable soul-searching until the debt is indeed quite
real. If we were honest with kids in high school, likely many of them would
tell college to fuck off while they figured out what it is they actually want to
do when they get out of bed every morning.

#Good Things

###<a name="writing"></a>Lots of practice writing
I got to write. A lot. This is easily the most valuable thing I got out of
college. Being beaten over the head with writing for four years has a way of
draining the drudgery of it. Assignments and papers that may have been tedious 
as a 16 year old in high school became routine, and raised my writing tolerance
to a level I am grateful to possess. I can write a lot in a sitting, and this
skill is invaluable to me as someone who still has the occasional idea.
I hope I became better writer. Doing something for 10,000 hours rarely makes
has the effect of decreasing one's ability.

###<a name="politics"></a>Practice navigating organizational politics
I was in a fair few organizations in college: newspaper, student government,
sports teams, clubs, etc. Every single one of became its own battlefield at one
time or another. Learning how to get things done despite competing egos, heinous external
factors, and team fatigue was a great primer for the shitshow that is adult
human beings interacting.

###<a name="rounded"></a>Well-roundedness
I spent 4 years studying a topic I have seldom used professionally. Aside from
the ludicrous privilege of this modern existence, this has given me a port with
which to view the world that is totally discrete from my other areas of
interest. Anyone reading this post probably knows me as someone interested in
the world of microcode (thanks, 
[Snow Crash](http://en.wikipedia.org/wiki/Snow_Crash)). The number of nights
I've spent discussing things like free software, privacy, and online rights is
dwarfed before the number I've spent drunkenly ingesting political Islam,
religion, ethics, and sexuality/gender.  I'm grateful for this. I love and feel
lucky that I have a mental life that complements what I do every day. I can at
least partially thank college for this chance.

###<a name="dissatisfaction"></a>Something to be dissatisfied with
One of the necessary conditions for adulthood is the realization
that one's elders do not by definition also compose one's betters. This is wildly on
display at most institutions of higher learning. Getting to see the mismanagement,
poor politicking, and outright incompetence of many of those tasked with stewarding
the next crop of engineers, doctors, and CEOs up close is one of the wonders of
the higher education world. Why am I counting this point as a "good thing"
instead of a "bad thing"? If capable, I try to ask myself, "Why do I find this
dissatisfying? How would I make it dissatisfying?" It's the essence of
improvement. For all the shit rightly given to student governments the world
over, we worked to improve campus drug/alcohol policy, protect student rights,
and improve facilities. This was born of our dissatisfaction with those who
would rather govern us [in loco parentis](http://en.wikipedia.org/wiki/In_loco_parentis).

###<a name="free-time"></a>Free time to pursue other stuff
The sales pitch of modern higher ed has come to include facts about the university's
student body participation in things like "ultimate frisbee" and "video game club".
Kidding aside, this is very probably one of the best things about college, and
part of what makes it such an opulence: the freedom to pursue things unrelated to
how one pays the bills. I got to pursue my love of news, journalism, and
photography, which lead directly to a future job in
[Tunisia](https://www.flickr.com/photos/clarkkampfe/sets/72157632276536045/).
